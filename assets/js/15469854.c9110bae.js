"use strict";(self.webpackChunkprominence=self.webpackChunkprominence||[]).push([[954],{3905:(e,n,t)=>{t.d(n,{Zo:()=>u,kt:()=>b});var r=t(7294);function i(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function o(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);n&&(r=r.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,r)}return t}function a(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?o(Object(t),!0).forEach((function(n){i(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):o(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function s(e,n){if(null==e)return{};var t,r,i=function(e,n){if(null==e)return{};var t,r,i={},o=Object.keys(e);for(r=0;r<o.length;r++)t=o[r],n.indexOf(t)>=0||(i[t]=e[t]);return i}(e,n);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(r=0;r<o.length;r++)t=o[r],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(i[t]=e[t])}return i}var l=r.createContext({}),p=function(e){var n=r.useContext(l),t=n;return e&&(t="function"==typeof e?e(n):a(a({},n),e)),t},u=function(e){var n=p(e.components);return r.createElement(l.Provider,{value:n},e.children)},m="mdxType",d={inlineCode:"code",wrapper:function(e){var n=e.children;return r.createElement(r.Fragment,{},n)}},c=r.forwardRef((function(e,n){var t=e.components,i=e.mdxType,o=e.originalType,l=e.parentName,u=s(e,["components","mdxType","originalType","parentName"]),m=p(t),c=i,b=m["".concat(l,".").concat(c)]||m[c]||d[c]||o;return t?r.createElement(b,a(a({ref:n},u),{},{components:t})):r.createElement(b,a({ref:n},u))}));function b(e,n){var t=arguments,i=n&&n.mdxType;if("string"==typeof e||i){var o=t.length,a=new Array(o);a[0]=c;var s={};for(var l in n)hasOwnProperty.call(n,l)&&(s[l]=n[l]);s.originalType=e,s[m]="string"==typeof e?e:i,a[1]=s;for(var p=2;p<o;p++)a[p]=t[p];return r.createElement.apply(null,a)}return r.createElement.apply(null,t)}c.displayName="MDXCreateElement"},5776:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>m,frontMatter:()=>o,metadata:()=>s,toc:()=>p});var r=t(7462),i=(t(7294),t(3905));const o={sidebar_position:1},a="Running jobs",s={unversionedId:"jobs/running-jobs",id:"jobs/running-jobs",title:"Running jobs",description:"Single node jobs",source:"@site/docs/jobs/running-jobs.md",sourceDirName:"jobs",slug:"/jobs/running-jobs",permalink:"/web/docs/jobs/running-jobs",draft:!1,tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1},sidebar:"tutorialSidebar",previous:{title:"MPI jobs",permalink:"/web/docs/containers/mpi-jobs"},next:{title:"Standard output & error",permalink:"/web/docs/jobs/stdout-stderr"}},l={},p=[{value:"Single node jobs",id:"single-node-jobs",level:2},{value:"Resources",id:"resources",level:2},{value:"MPI jobs",id:"mpi-jobs",level:2},{value:"OpenMP",id:"openmp",level:2},{value:"Working directory",id:"working-directory",level:2},{value:"Environment variables",id:"environment-variables",level:2},{value:"User-provided",id:"user-provided",level:3},{value:"Default",id:"default",level:3}],u={toc:p};function m(e){let{components:n,...t}=e;return(0,i.kt)("wrapper",(0,r.Z)({},u,t,{components:n,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"running-jobs"},"Running jobs"),(0,i.kt)("h2",{id:"single-node-jobs"},"Single node jobs"),(0,i.kt)("p",null,"In order to run an instance of a container, running the command defined in the image\u2019s entrypoint, all you need to do is to specify the Docker Hub image name:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"$ prominence create eoscprominence/testpi\nJob created with id 3101\n")),(0,i.kt)("p",null,"When a job has been successfully submitted an (integer) ID will be returned. Alternatively, a command (and arguments) can be specified. For example:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},'$ prominence create centos:7 "/bin/sleep 100"\n')),(0,i.kt)("p",null,"The command of course should exist within the container. If arguments need to be specified you should put the command and any arguments inside a single set of double quotes, as in the example above."),(0,i.kt)("p",null,"To run multiple commands inside the same container, use ",(0,i.kt)("inlineCode",{parentName:"p"},"/bin/bash -c")," with the commands enclosed in quotes and seperated by semicolons, for example:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},'$ prominence create centos:7 "/bin/bash -c \\"date; sleep 10; date\\""\n')),(0,i.kt)("p",null,"This is of course assuming ",(0,i.kt)("inlineCode",{parentName:"p"},"/bin/bash")," exists inside the container image."),(0,i.kt)("h2",{id:"resources"},"Resources"),(0,i.kt)("p",null,"By default a job will be run with 1 CPU and 1 GB memory but this can easily be changed. The following resources can be specified:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"CPU cores"),(0,i.kt)("li",{parentName:"ul"},"Memory (in GB)"),(0,i.kt)("li",{parentName:"ul"},"Disk (in GB)"),(0,i.kt)("li",{parentName:"ul"},"Maximum runtime (in mins)")),(0,i.kt)("p",null,"CPU cores and memory can be specified using the ",(0,i.kt)("inlineCode",{parentName:"p"},"--cpus")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"--memory")," options. A disk size can also be specified using ",(0,i.kt)("inlineCode",{parentName:"p"},"--disk"),"."),(0,i.kt)("p",null,"Here is an example running an MPI job on 4 nodes where each node has 2 CPUs and 8 GB memory, there is a shared 20 GB disk accessible by all 4 nodes, and the maximum runtime is 1000 minutes:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"$ prominence create --openmpi \\\n                    --nodes 4 \\\n                    --cpus 2 \\\n                    --memory 8 \\\n                    --disk 20 \\\n                    --runtime 1000 \\\n                    alahiff/geant4mpi:1.3a3\n")),(0,i.kt)("p",null,"By default a 10 GB disk is available to jobs, which is located on separate block storage. For MPI jobs the disk is available across all nodes running the job. The default maximum runtime is 720 minutes."),(0,i.kt)("h2",{id:"mpi-jobs"},"MPI jobs"),(0,i.kt)("p",null,"To run an MPI job, you need to specify either ",(0,i.kt)("inlineCode",{parentName:"p"},"--openmpi")," for Open MPI, ",(0,i.kt)("inlineCode",{parentName:"p"},"--intelmpi")," for Intel MPI and ",(0,i.kt)("inlineCode",{parentName:"p"},"--mpich")," for MPICH. For multi-node jobs the number of nodes required should also be specified. For example:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"$ prominence create --openmpi --nodes 4 alahiff/openmpi-hello-world:latest /mpi_hello_world\n")),(0,i.kt)("p",null,"The number of processes to run per node is assumed to be the same as the number of cores available per node. If the number of cores available per node is more than the requested number of cores all cores will be used. This behaviour can be changed by using ",(0,i.kt)("inlineCode",{parentName:"p"},"--procs-per-node")," to define the number of processes per node to use."),(0,i.kt)("admonition",{type:"tip"},(0,i.kt)("p",{parentName:"admonition"},"It is always preferable to run MPI jobs within a single node if possible, especially for low numbers of CPU cores.")),(0,i.kt)("p",null,"For MPI jobs a command to run (and optionally any arguments) must be specified. If an entrypoint is defined in the container image it will be ignored."),(0,i.kt)("h2",{id:"openmp"},"OpenMP"),(0,i.kt)("p",null,"In this situation the number of MPI processes to run per node must be specified using ",(0,i.kt)("inlineCode",{parentName:"p"},"--procs-per-node")," and the environment variable OMP_NUM_THREADS should be set to the required number of OpenMP threads per MPI process."),(0,i.kt)("p",null,"In the following example we have 2 nodes with 4 CPUs each, and we run 2 MPI processes on each node, where each MPI process runs 2 OpenMP threads:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},'$ prominence create --cpus 4 \\\n                    --memory 4 \\\n                    --nodes 2 \\\n                    --procs-per-node 2 \\\n                    --openmpi \\\n                    --env OMP_NUM_THREADS=2 \\\n                    --artifact https://github.com/lammps/lammps/archive/stable_12Dec2018.tar.gz \\\n                    --workdir lammps-stable_12Dec2018/bench \\\n                    alahiff/lammps-openmpi-omp "lmp_mpi -sf omp -in in.lj"\n')),(0,i.kt)("h2",{id:"working-directory"},"Working directory"),(0,i.kt)("p",null,"By default the current working directory is scratch space made available inside the container. The path to this directory is also specified by the environment variables HOME, TEMP and TMP."),(0,i.kt)("p",null,"To specify a different working directory use ",(0,i.kt)("inlineCode",{parentName:"p"},"--workdir"),". For example, the following will run ",(0,i.kt)("inlineCode",{parentName:"p"},"pwd"),' inside the "/tmp" directory.'),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"$ prominence create --workdir /tmp centos:7 pwd\n")),(0,i.kt)("admonition",{type:"note"},(0,i.kt)("p",{parentName:"admonition"},"Remember that you should not try to write inside the container\u2019s filesystem as this may be prevented by the container runtime or result in permission problems.")),(0,i.kt)("h2",{id:"environment-variables"},"Environment variables"),(0,i.kt)("h3",{id:"user-provided"},"User-provided"),(0,i.kt)("p",null,"It is a common technique to use environment variables to pass information, such as configuration options, into a container. The option ",(0,i.kt)("inlineCode",{parentName:"p"},"--env")," can be used to specify an environment variable in the form of a key-value pair separated by \u201c=\u201d. This option can be specified multiple times to set multiple environment variables. For example:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"$ prominence create --env LOWER=4.5 --env UPPER=6.7 test/container\n")),(0,i.kt)("h3",{id:"default"},"Default"),(0,i.kt)("p",null,"Some environment variables are set automatically and are available for jobs to use."),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"PROMINENCE_CPUS"),": the number of CPUs available, which could be larger than what was requested"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"PROMINENCE_MEMORY"),": the amount of memory in GB available, which could be larger than what was requested"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"PROMINENCE_CONTAINER_RUNTIME"),": the container runtime in use, either ",(0,i.kt)("inlineCode",{parentName:"li"},"singularity")," or ",(0,i.kt)("inlineCode",{parentName:"li"},"udocker")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"PROMINENCE_JOB_ID"),": the id of the job"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"PROMINENCE_WORKFLOW_ID"),": the id of the associated workflow, if applicable"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"PROMINENCE_URL"),": URL of the PROMINENCE REST API"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"PROMINENCE_TOKEN"),": token which can be used to authenticate against the PROMINENCE REST API (a unique token is generated per job, and is valid for the lifetime of the job)")))}m.isMDXComponent=!0}}]);